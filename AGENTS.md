# AGENTS.md - AI Agent Instructions for tldr-swinton

This document provides instructions for all AI coding assistants (Claude, Codex, etc.) working with the tldr-swinton codebase.

**Quick start for agents**: Run `tldrs quickstart` for a concise reference guide.

## Project Overview

tldr-swinton is a token-efficient code analysis tool for LLMs. It's a fork of llm-tldr with fixes for TypeScript and Rust support.

**Key directories:**
- `src/tldr_swinton/` - Main Python package
- `evals/` - Evaluation scripts for token efficiency

## Related Projects

| Project | What | Path |
|---------|------|------|
| **Ashpool** | Agent workbench: run capture + artifact store + eval/regression for tldrs outputs | `../Ashpool` |

**Ashpool integration**: Ashpool captures tldrs runs as artifacts with metadata tagging, scores token efficiency, and A/B tests output formats. When adding new tldrs formats or flags, 4 files must be kept in sync:
- `../Ashpool/scripts/regression_suite.json` — regression queries for each (command, format, flag) combination
- `../Ashpool/scripts/ab_formats.py` — `DEFAULT_FORMATS` list for A/B testing
- `../Ashpool/demo-tldrs.sh` — demo runs showcasing each format
- `../Ashpool/scripts/score_tokens.py` — `parse_*` functions for scoring_hints formats

**Sync workflow** (automated):
```bash
# Check for gaps between tldrs capabilities and Ashpool coverage
tldrs manifest | python3 ../Ashpool/scripts/check_tldrs_sync.py

# Or use the Claude Code skill for guided remediation
/tldrs-ashpool-sync
```

The `tldrs manifest` command produces a machine-readable JSON of all eval-relevant commands, formats, flags, and scoring hints. The sync check script reads this and reports coverage gaps across all 4 Ashpool files. The `bump-version.sh` script runs the sync check automatically and warns if gaps exist.

## Quick Reference

```bash
# Install (development)
uv pip install -e .
uv pip install -e ".[semantic-ollama]"  # Ollama-only semantic search (FAISS + NumPy)
uv pip install -e ".[semantic]"  # With sentence-transformers fallback (includes torch)
uv pip install -e ".[full]"      # Full stack (includes Ollama + tiktoken)
uv pip install -e ".[structural]"  # Structural code search (ast-grep)

# Test commands
tldrs extract src/tldr_swinton/embeddings.py    # Extract file info
tldrs structure src/                             # Show code structure
tldrs find "authentication logic"                # Semantic search (requires index)
tldrs index .                                    # Build semantic index

# After code changes
find . -name "*.pyc" -delete && find . -name "__pycache__" -type d -exec rm -rf {} +
uv pip install -e .
```

Full workflow guide: see `docs/agent-workflow.md` or run `tldrs quickstart`.

## Dev Quickstart

```bash
# From repo root
uv pip install -e ".[full]"
tldrs --help

# Smoke check
tldrs extract src/tldr_swinton/embeddings.py
```

## Claude Code Plugin

The repo includes a Claude Code plugin at `.claude-plugin/`. Available commands:

| Command | Description |
|---------|-------------|
| `/tldrs-find <query>` | Semantic code search |
| `/tldrs-diff` | Diff-focused context for recent changes |
| `/tldrs-context <symbol>` | Symbol-level context |
| `/tldrs-structural <pattern>` | Structural code search (ast-grep patterns) |
| `/tldrs-quickstart` | Show quick reference guide |

**Autonomous skills** (Claude invokes automatically based on task context):

| Skill | Trigger |
|-------|---------|
| `tldrs-session-start` | Before reading code for bugs, features, refactoring, tests, reviews, migrations |
| `tldrs-find-code` | Searching for code by concept, pattern, or text |
| `tldrs-understand-symbol` | Understanding how a function/class works, its callers, dependencies |
| `tldrs-ashpool-sync` | Syncing Ashpool eval coverage after tldrs capability changes |

**Hooks:**
- `PreToolUse` on **Read** and **Grep**: Suggests running tldrs recon before reading files (once per session via flag file)
- `SessionStart` (setup.sh): Checks tldrs install, semantic index, ast-grep availability

To use as a plugin:
```bash
/plugin install tldr-swinton   # From interagency-marketplace
```

## Codex Skill (Repo-Scoped)

The repo includes a Codex skill at:
`./.codex/skills/tldrs-agent-workflow/`

This skill mirrors `docs/agent-workflow.md` and is intended for agent onboarding.

## tldr-bench Datasets (Submodule)

Official benchmark datasets live in the `tldr-bench/data` submodule and are
stored in a separate repo: `https://github.com/mistakeknot/tldr-bench-datasets`.

Setup:
```bash
git submodule update --init --recursive
cd tldr-bench/data
git lfs install
git lfs pull
cd -
```

Dataset files are under `tldr-bench/data/data/`. Do not add large dataset files
directly to this repo; update the datasets repo instead and bump the submodule.

## Agent Workflow Checklist

- Read `docs/agent-workflow.md` first.
- Start with `tldrs diff-context --project . --budget 2000`.
- Use `tldrs context <entry>` with `--format ultracompact` and a budget.
- Use `tldrs structure` or `tldrs extract` to discover symbols before context.
- Only open full files when making edits.

## ContextPack Notes (Dev)

- `tldrs context --format json` returns ContextPack JSON (slices; signature-only slices have `code: null`).
- Each ContextPack slice now includes `etag` (signature + code hash).
- Ambiguous entries return candidate lists; re-run with `file.py:func`.
- For API tests, use `get_symbol_context_pack(..., etag=...)` to get `"UNCHANGED"`.

## Delta Context Mode (Multi-Turn Token Savings)

Delta mode tracks which symbols have been delivered to an LLM session and skips
re-sending unchanged code on subsequent calls. This can provide **~60% token savings**
in multi-turn conversations **when code is unchanged between calls**.

**Important caveats:**
- Delta savings collapse to near-zero if code changes between calls
- The 60% figure is conditional on unchanged code - actual savings vary
- Delta mode is most valuable for iterative Q&A, not active editing sessions

### Where Delta Mode Works

- **`diff-context`** (recommended): Full code bodies included, delta mode provides
  real token savings. Use `--session-id <id>` or `--delta` flag.
- **`context`**: Signatures-only by design (95% savings already), delta mode adds
  `[UNCHANGED]` markers but doesn't reduce output size significantly.

### Usage

```bash
# First call - full output, records deliveries
tldrs diff-context --project . --session-id my-session

# Second call - unchanged symbols have code omitted
tldrs diff-context --project . --session-id my-session
# Shows: "Delta: 134 unchanged, 0 changed (100% cache hit)"

# Auto-generate session ID
tldrs diff-context --project . --delta

# Disable delta even with session-id
tldrs diff-context --project . --session-id my-session --no-delta
```

### Design Rationale

The standard `context` command returns signatures-only. Adding code bodies would
defeat the purpose. Delta mode is most valuable with `diff-context` which includes
full code for changed files.

**Note:** The "95% token savings" claim compares signatures to full files. In practice,
agents need full code for editing, so real-world savings for editing workflows are
typically **20-35%** compared to naive file reading approaches.

Session state is stored in `.tldrs/state.sqlite3` and tracks:
- Session ID, repo fingerprint, language
- Per-symbol deliveries with etags (sha256 of signature+code)
- Sessions expire after 24 hours of inactivity

## MCP (Optional)

- MCP server requires `uv pip install mcp`.
- MCP context uses ContextPack for `json/json-pretty/ultracompact` formats.

## Output Caps (`--max-lines` / `--max-bytes`)

The `context`, `diff-context`, and `slice` commands support optional post-format
output truncation. These are opt-in — no behavior change without them.

```bash
# Cap context output to 20 lines
tldrs context main --project . --max-lines=20

# Cap diff-context to 4KB
tldrs diff-context --project . --max-bytes=4096

# Both caps together
tldrs context main --project . --max-lines=50 --max-bytes=2048

# Slice with line cap
tldrs slice src/app.py handle_request 42 --max-lines=10
```

When output is truncated, a marker is appended:
`[TRUNCATED: output exceeded --max-lines=20]`

For JSON output (`slice`), the `lines`/`slices` array is trimmed from the end
and `"truncated": true` is added to the result dict. Output remains valid JSON.

## Module Selection (Agents)

Preferred order when gathering context:

```bash
# 1) Diff-first context for recent changes
tldrs diff-context --project . --budget 2000

# 2) Symbol-level context for a specific entry
tldrs context <entry> --project . --depth 2 --budget 2000 --format ultracompact

# 3) Structure / extract for files or folders
tldrs structure src/
tldrs extract path/to/file.py

# 4) Semantic search (requires index)
tldrs index .
tldrs find "authentication logic"

# 5) Structural code search (requires ast-grep-py)
tldrs structural 'def $FUNC($$$ARGS): return None' --lang python

# 6) Deep analysis helpers (optional)
tldrs slice <file> <func> <line>
tldrs cfg <file> <function>
tldrs dfg <file> <function>

# 7) Machine-readable capability manifest (for tooling/eval sync)
tldrs manifest --pretty
```

For large outputs, store as VHS refs:

```bash
tldrs context <entry> --project . --output vhs
tldrs context <entry> --project . --include vhs://<hash>
```

If `tldrs-vhs` isn't on PATH (non-interactive shells), set:

```bash
export TLDRS_VHS_CMD="$HOME/tldrs-vhs/.venv/bin/tldrs-vhs"
```

## Compression Modes

The `--compress` flag on `diff-context` supports two experimental modes:

- **`two-stage`**: Indent-aware block detection + 0/1 knapsack DP for block selection.
  Scores blocks by diff overlap (10x), adjacency (3x), and control-flow keywords (0.5x).
  Saves 35-73% tokens with `--budget` constraint.
- **`chunk-summary`**: Replaces code with LLM-ready summaries (signature + key lines).
  Saves 85-95% but loses implementation detail.

### Promotion Gate

Experimental compression modes **must not be promoted to default** until they pass:
- **>=10% additional savings** vs the current diff+deps baseline on `evals/difflens_eval.py`
- **No regressions** on `evals/agent_workflow_eval.py`
- **At least one manual spot check** on a real repo (correctness/readability)

## Architecture

### Core Extraction Pipeline

```
CLI (cli.py)
    ↓
API layer (api.py)
    ↓
extract_file() → HybridExtractor.extract()
    ↓
Language-specific extraction:
  - Python: Native AST (ast_extractor.py)
  - TypeScript/Rust/Go/etc: tree-sitter (hybrid_extractor.py)
  - Fallback: Pygments signatures
    ↓
ModuleInfo with FunctionInfo objects
    ↓
.to_dict() for JSON serialization
```

### Semantic Search Pipeline (v0.2.0)

```
tldrs index .  → embeddings.py (Ollama/sentence-transformers)
                      ↓
               vector_store.py (FAISS IndexFlatIP)
                      ↓
               .tldrs/index/{vectors.faiss, units.json, meta.json}

tldrs find "query" → index.py:search_index() → VectorStore.search()
                          ↓
                    Lexical fast-path for identifiers (exact match)
                          ↓
                    Semantic search for natural language queries
```

### Key Data Structures

**FunctionInfo** (`ast_extractor.py:27`):
```python
@dataclass
class FunctionInfo:
    name: str
    params: list[str]
    return_type: str | None
    docstring: str | None
    is_method: bool = False
    is_async: bool = False
    decorators: list[str] = field(default_factory=list)
    line_number: int = 0
    language: str = "python"  # Controls signature() output format
```

The `language` field is critical - it determines the output format of `signature()`:
- `"typescript"` → `async function name(params): Type`
- `"rust"` → `fn name(params) -> Type`
- `"python"` (default) → `def name(params) -> Type`

**ModuleInfo** (`ast_extractor.py:131`): Container for file analysis results.

**CodeUnit** (`vector_store.py`): Minimal metadata for semantic search results.

### Language Support

Language detection happens in multiple places:
1. `cli.py:EXTENSION_TO_LANGUAGE` - Maps file extensions to language names
2. `hybrid_extractor.py:_detect_language()` - Runtime detection from extension
3. `api.py:get_code_structure()` - Auto-detection for single files

## Critical Rules

### Import Convention

All internal imports MUST use relative imports:
```python
# CORRECT
from .hybrid_extractor import HybridExtractor
from .ast_extractor import FunctionInfo

# WRONG - imports from old llm-tldr package!
from tldr.hybrid_extractor import HybridExtractor
```

### Language Field Required

When creating `FunctionInfo` objects for non-Python languages, ALWAYS set the `language` field:
```python
FunctionInfo(
    name=func_name,
    params=params,
    return_type=return_type,
    docstring=None,
    language="typescript",  # REQUIRED for correct signature format
)
```

### Function Name Cleaning

TypeScript/JavaScript function names must be cleaned of modifiers:
```python
for prefix in ("export ", "async ", "default "):
    if name.startswith(prefix):
        name = name[len(prefix):]
```

### Embeddings Must Be L2-Normalized

FAISS IndexFlatIP expects normalized vectors for cosine similarity:
```python
embedding = embedding / np.linalg.norm(embedding)  # Required!
```

### Incremental Index Updates

`build_index()` reconstructs vectors for unchanged files (via `store.reconstruct_all_vectors()`). Only new/changed files get re-embedded.

## Common Tasks

### Adding a New Language

1. Add tree-sitter grammar to `pyproject.toml` dependencies
2. Add extension mapping in `cli.py:EXTENSION_TO_LANGUAGE`
3. Add signature format in `FunctionInfo.signature()` (`ast_extractor.py`)
4. Add extraction method in `HybridExtractor` (`hybrid_extractor.py`)
5. Add to language maps in `api.py:get_code_structure()`

### Fixing Signature Formatting

The `FunctionInfo.signature()` method (`ast_extractor.py:39-84`) handles all signature formatting. Each language has a branch:

```python
if self.language in ("typescript", "tsx", "javascript"):
    return f"{async_prefix}function {self.name}({params_str}){ret}"
elif self.language == "rust":
    return f"{async_prefix}fn {self.name}({params_str}){ret_rust}"
# ... etc
```

### Semantic Search Backends

```bash
# Check available backends
python -c "from tldr_swinton.embeddings import check_backends; print(check_backends())"

# Ollama (preferred - fast, local)
ollama pull nomic-embed-text-v2-moe
tldrs index . --backend ollama

# sentence-transformers (fallback - 1.3GB download)
tldrs index . --backend sentence-transformers

# Auto (tries Ollama first, falls back)
tldrs index . --backend auto
```

## Debugging

### Verify Correct Module Loaded

```bash
python -c "import tldr_swinton; print(tldr_swinton.__file__)"
python -c "from tldr_swinton.hybrid_extractor import HybridExtractor; import inspect; print(inspect.getfile(HybridExtractor))"
```

### Test Extraction Directly

```bash
python -c "
from tldr_swinton.hybrid_extractor import HybridExtractor
e = HybridExtractor()
r = e.extract('path/to/file.ts')
for f in r.functions[:3]:
    print(f'{f.name}: {f.language} -> {f.signature()}')
"
```

### Check for Import Issues

```bash
grep -r "from tldr\." src/tldr_swinton/ --include="*.py"
```
Should return empty - all imports should be relative (`.`).

### Check Index Health

```bash
tldrs index --info                    # Show index stats
ls -la .tldrs/index/                   # Check index files exist
```

## Testing

### Manual Testing

```bash
# TypeScript - should show "function" not "def"
tldrs extract path/to/file.ts | grep signature

# Rust - should show "fn" not "def"
tldrs extract path/to/file.rs | grep signature

# Single file structure - should work and detect language
tldrs structure path/to/file.ts | grep language
```

### After Making Changes

1. Clear Python cache:
   ```bash
   find . -name "*.pyc" -delete
   find . -name "__pycache__" -type d -exec rm -rf {} +
   ```

2. Reinstall in development mode:
   ```bash
   uv pip install -e .
   ```

3. Verify the correct module is loaded:
   ```bash
   python -c "import tldr_swinton; print(tldr_swinton.__file__)"
   ```

### Evals

```bash
# Token efficiency eval (basic)
python evals/token_efficiency_eval.py

# Semantic search eval
python evals/semantic_search_eval.py

# Agent workflow eval (realistic Claude Code scenarios)
python evals/agent_workflow_eval.py
```

The agent workflow eval tests real token savings for code modification tasks (not just search output vs raw code).

## File Reference

| File | Purpose |
|------|---------|
| `cli.py` | CLI entry point, argument parsing |
| `api.py` | High-level API functions |
| `ast_extractor.py` | Data structures, Python extraction |
| `hybrid_extractor.py` | Multi-language extraction via tree-sitter |
| `cfg_extractor.py` | Control flow graph extraction |
| `dfg_extractor.py` | Data flow graph extraction |
| `pdg_extractor.py` | Program dependency graph |
| `signature_extractor_pygments.py` | Fallback signature extraction |
| `embeddings.py` | Ollama/sentence-transformers embedding backend |
| `vector_store.py` | FAISS vector storage wrapper |
| `index.py` | Semantic index management |
| `engines/astgrep.py` | Structural code search via ast-grep |
| `engines/delta.py` | Delta-mode orchestration (session tracking, etag comparison) |
| `manifest.py` | Machine-readable capability manifest for eval sync |
| `bm25_store.py` | BM25 keyword index for hybrid search |
| `semantic.py` | Original semantic search (5-layer embeddings) |

## Version History

- **0.6.2** - Ashpool sync automation, plugin effectiveness improvements
  - Added `tldrs manifest` — machine-readable JSON of all eval-relevant capabilities
  - Added `/tldrs-ashpool-sync` skill for guided Ashpool eval coverage sync
  - Added `check_tldrs_sync.py` sync check script (reads manifest, reports gaps in 4 Ashpool files)
  - Broadened skill triggers to match more task types (debug, refactor, tests, migrate)
  - Added Grep `PreToolUse` hook (same suggest-recon as Read hook)
  - Setup hook now emits usage guidance for any repo
  - `bump-version.sh` warns if Ashpool coverage has gaps

- **0.6.1** - Delta engine extraction, parser caching
  - Extracted delta-mode orchestration from `cli.py` into `engines/delta.py`
  - Cached tree-sitter parsers in `cross_file_calls.py` via `@lru_cache`

- **0.6.0** - Wave 2+3 features wired into pipeline
  - Wired Wave 2 and Wave 3 prompt-cache-friendly features into the output pipeline

- **0.5.0** - Skill-first plugin, structural search, compression upgrades
  - Plugin restructured: 4 focused skills replace 1 broad skill
  - Added `tldrs structural` - ast-grep tree-sitter pattern matching
  - Added `@lru_cache` to 14 tree-sitter parser factory functions
  - Upgraded `_two_stage_prune` with indent-based blocks + knapsack DP
  - Added BM25 hybrid search (RRF fusion with semantic search)
  - Upgraded embedding model to `nomic-embed-text-v2-moe` (475M MoE)
  - Added `--max-lines` / `--max-bytes` output caps
  - New optional dep group: `[structural]` for ast-grep-py

- **0.4.0** - Output caps, benchmark infrastructure
  - Added `--max-lines` / `--max-bytes` to context, diff-context, slice
  - Added benchmark harness (`tldrs bench`)
  - Disabled non-demonstrable benchmark variants

- **0.3.0** - Embedding and search upgrades
  - Switched Ollama model to `nomic-embed-text-v2-moe`
  - Added BM25 hybrid search with RRF fusion
  - Parallelized Ollama embedding calls

- **0.2.0** - Semantic search with Ollama support
  - Added `tldrs index` - Build semantic index with Ollama or sentence-transformers
  - Added `tldrs find` - Natural language code search
  - Added `embeddings.py` - Multi-backend embedding support (Ollama/HuggingFace)
  - Added `vector_store.py` - FAISS wrapper with persistent storage
  - Added `index.py` - Index management with incremental updates
  - Index stored in `.tldrs/index/` (vectors.faiss, units.json, meta.json)

- **0.1.0** - Initial fork with TypeScript/Rust signature fixes
  - Fixed `FunctionInfo.signature()` to be language-aware
  - Fixed function name cleaning for TypeScript
  - Fixed single file support in `structure` command
  - Fixed internal imports (tldr → tldr_swinton)
